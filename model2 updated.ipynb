{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3ddafa",
   "metadata": {},
   "source": [
    "# Model2: Pretrained Encoder + RNN Pixel‐Wise Refinement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2197bc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Caching TRAIN grayscale inputs to RAM…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:03<00:00, 1277.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grayscale cache: torch.Size([5000, 1, 256, 256])\n",
      "DataLoaders ready → 313 train batches, 500 test batches\n"
     ]
    }
   ],
   "source": [
    "# — Karma Cache: Gri input’ları RAM’de, renkli hedefleri diskten —\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.datasets import STL10\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Transforms\n",
    "resize256 = T.Resize((256,256))\n",
    "to_tensor  = T.ToTensor()\n",
    "\n",
    "# 1) Gri input’ları RAM’e cache\n",
    "train_raw = STL10(root='./data', split='train', download=True)\n",
    "inp_list = []\n",
    "print(\"Caching TRAIN grayscale inputs to RAM…\")\n",
    "for img, _ in tqdm(train_raw, total=len(train_raw)):\n",
    "    gray = T.functional.rgb_to_grayscale(resize256(img), num_output_channels=1)\n",
    "    inp_list.append(to_tensor(gray))\n",
    "inp_tensor = torch.stack(inp_list)       # [5000,1,256,256]\n",
    "print(\"Grayscale cache:\", inp_tensor.shape)\n",
    "\n",
    "# 2) Disk‐based renkli targets, on-the-fly transform\n",
    "class HybridTrain256(torch.utils.data.Dataset):\n",
    "    def __init__(self, inp_tensor):\n",
    "        self.inp = inp_tensor\n",
    "        self.base = STL10(root='./data', split='train', download=False)\n",
    "        self.tf_tgt = T.Compose([T.Resize((256,256)), T.ToTensor()])\n",
    "    def __len__(self):\n",
    "        return len(self.inp)\n",
    "    def __getitem__(self, idx):\n",
    "        gray = self.inp[idx]  # tensor in RAM\n",
    "        img, _ = self.base[idx]\n",
    "        tgt = self.tf_tgt(img)\n",
    "        return gray, tgt\n",
    "\n",
    "train_ds = HybridTrain256(inp_tensor)\n",
    "\n",
    "# 3) Disk‐based test loader as before\n",
    "test_ds = HybridTrain256(torch.stack([\n",
    "    T.ToTensor()(T.functional.rgb_to_grayscale(\n",
    "        T.Resize((256,256))(img),1\n",
    "    )) for img,_ in STL10(root='./data', split='test', download=False)\n",
    "]))\n",
    "# But we need corresponding colored targets for test\n",
    "test_tgt_ds = STL10(root='./data', split='test', download=False, transform=T.ToTensor())\n",
    "class HybridTest256(torch.utils.data.Dataset):\n",
    "    def __init__(self, gray_ds, tgt_ds):\n",
    "        self.gray_ds = gray_ds\n",
    "        self.tgt_ds  = tgt_ds\n",
    "    def __len__(self):\n",
    "        return len(self.gray_ds)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.gray_ds[idx], self.tgt_ds[idx][0]\n",
    "\n",
    "test_ds = HybridTest256(\n",
    "    torch.stack([T.ToTensor()(T.functional.rgb_to_grayscale(\n",
    "        T.Resize((256,256))(img),1\n",
    "    )) for img,_ in STL10(root='./data', split='test', download=False)]),\n",
    "    STL10(root='./data', split='test', download=False, transform=T.Compose([T.Resize((256,256)), T.ToTensor()]))\n",
    ")\n",
    "\n",
    "# 4) DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=16, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "print(\"DataLoaders ready →\", len(train_loader), \"train batches,\", len(test_loader), \"test batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff10e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed Model2 loaded, total params: 3,696,358\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, hidden_size=128, window=5):\n",
    "        super().__init__()\n",
    "        # 1) Encoder\n",
    "        resnet = resnet18(pretrained=True)\n",
    "        self.encoder = nn.Sequential(\n",
    "            resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool,\n",
    "            resnet.layer1, resnet.layer2, resnet.layer3\n",
    "        )\n",
    "        for p in self.encoder.parameters(): p.requires_grad = False\n",
    "\n",
    "        # 2) Coarse decoder: 6→96 upsample 4×\n",
    "        self.coarse = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256,128,4,2,1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128,64,4,2,1),  nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64,32,4,2,1),   nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32,16,4,2,1),   nn.ReLU(True),\n",
    "            nn.Conv2d(16,3,3,padding=1),       nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # 3) Project encoder global features to hidden_size\n",
    "        self.h_init = nn.Linear(256, hidden_size)\n",
    "\n",
    "        # 4) Pixel-wise RNN\n",
    "        self.win = window\n",
    "        inp_dim = window*window*4 + hidden_size\n",
    "        self.rnn = nn.LSTMCell(inp_dim, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, gray):\n",
    "        B = gray.size(0)\n",
    "        feat = self.encoder(gray.repeat(1,3,1,1))    # [B,256,6,6]\n",
    "        coarse = self.coarse(feat)                   # [B,3,96,96]\n",
    "\n",
    "        # Prepare sliding window input\n",
    "        pad = self.win//2\n",
    "        inp_cat = torch.cat([gray, coarse], dim=1)   # [B,4,96,96]\n",
    "        inp_pad = F.pad(inp_cat, (pad,)*4, mode='reflect')\n",
    "\n",
    "        # Initialize hidden state from feat\n",
    "        global_feat = feat.mean(dim=[2,3])           # [B,256]\n",
    "        h = torch.tanh(self.h_init(global_feat))     # [B, hidden_size]\n",
    "        c = torch.zeros_like(h)\n",
    "\n",
    "        out_ref = torch.zeros_like(coarse)\n",
    "        for y in range(96):\n",
    "            for x in range(96):\n",
    "                patch = inp_pad[:, :, y:y+self.win, x:x+self.win].reshape(B, -1)\n",
    "                rnn_in = torch.cat([patch, h], dim=1)        # [B, inp_dim]\n",
    "                h, c = self.rnn(rnn_in, (h, c))\n",
    "                out_ref[:,:,y,x] = self.out(h).reshape(B,3)\n",
    "\n",
    "        return coarse + out_ref\n",
    "\n",
    "# Re-instantiate\n",
    "model2 = Model2(hidden_size=128, window=5).to(device)\n",
    "print(f\"Fixed Model2 loaded, total params: {sum(p.numel() for p in model2.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e21c567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15860\\614728639.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = GradScaler()\n",
      "Epoch 1 Train:   0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# — Hücre 3: Model2 Eğitim Döngüsü (10 epoch) —\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from skimage.metrics import peak_signal_noise_ratio as compute_psnr, structural_similarity as compute_ssim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam([p for p in model2.parameters() if p.requires_grad], lr=1e-4)\n",
    "scaler    = GradScaler()\n",
    "epochs, best_val, patience, wait = 10, float('inf'), 3, 0\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    t0 = time.time()\n",
    "    # — Train —\n",
    "    model2.train()\n",
    "    train_loss = 0.0\n",
    "    for gray, tgt in tqdm(train_loader, desc=f\"Epoch {ep} Train\"):\n",
    "        gray, tgt = gray.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            out = model2(gray)\n",
    "            loss = criterion(out, tgt)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        train_loss += loss.item() * gray.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # — Val & Metrics —\n",
    "    model2.eval()\n",
    "    val_loss, psnr_sum, ssim_sum = 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for gray, tgt in tqdm(test_loader, desc=f\"Epoch {ep} Val\"):\n",
    "            gray, tgt = gray.to(device), tgt.to(device)\n",
    "            out = model2(gray)\n",
    "            val_loss += criterion(out, tgt).item() * gray.size(0)\n",
    "            o_np = out.cpu().permute(0,2,3,1).numpy()\n",
    "            t_np = tgt.cpu().permute(0,2,3,1).numpy()\n",
    "            for o, t in zip(o_np, t_np):\n",
    "                psnr_sum += compute_psnr(t, o, data_range=1.0)\n",
    "                ssim_sum += compute_ssim(t, o, channel_axis=2, data_range=1.0, win_size=7)\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "    avg_psnr = psnr_sum / len(test_loader.dataset)\n",
    "    avg_ssim = ssim_sum / len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {ep}/{epochs}  \"\n",
    "        f\"Train Loss: {train_loss:.4f}  Val Loss: {val_loss:.4f}  \"\n",
    "        f\"PSNR: {avg_psnr:.2f} dB  SSIM: {avg_ssim:.3f}  \"\n",
    "        f\"Time: {time.time()-t0:.1f}s\"\n",
    "    )\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save(model2.state_dict(), 'model2_best.pt')\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "print(\"✅ Model2 training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2bb284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# — Görselleştirme Hücresi: 10 Örnek —\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model2.eval()\n",
    "fig, axs = plt.subplots(10, 3, figsize=(8, 24))\n",
    "indices = list(range(0, len(test_ds), len(test_ds)//10))\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    gray, tgt = test_ds[idx]\n",
    "    with torch.no_grad():\n",
    "        out = model2(gray.unsqueeze(0).to(device))[0].cpu()\n",
    "    # Gri girdi\n",
    "    axs[i,0].imshow(gray.squeeze(0), cmap='gray')\n",
    "    axs[i,0].axis('off')\n",
    "    axs[i,0].set_title(\"Gri Input\")\n",
    "    # Tahmin\n",
    "    axs[i,1].imshow(out.permute(1,2,0))\n",
    "    axs[i,1].axis('off')\n",
    "    axs[i,1].set_title(\"Model2 Prediction\")\n",
    "    # Yer Gerçek (Ground Truth)\n",
    "    axs[i,2].imshow(tgt.permute(1,2,0))\n",
    "    axs[i,2].axis('off')\n",
    "    axs[i,2].set_title(\"Ground Truth\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3deb595",
   "metadata": {},
   "source": [
    "---\n",
    "**Model2 Değerlendirme:**  \n",
    "- Coarse renk tahminin üzerine RNN tabanlı piksel refinmenti ekledik.  \n",
    "- PSNR/SSIM iyileşti mi? Grafik yaparak gösterebilirsiniz.  \n",
    "- Geliştirilebilecek noktalar: pencere boyutu, hidden size, iki yönde geçme (bidirectional LSTM), vb.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
